
Recent change stream:
	https://stream.wikimedia.org/v2/stream/recentchange
Demo at:	
	https://esjewett.github.io/wm-eventsource-demo/
	https://codepen.io/Krinkle/pen/BwEKgW?editors=1010
Use Java Libraries
	OKhttp3
		implementation 'com.squareup.okhttp3:okhttp:4.9.3'
	Okhttp-eventsource
	    implementation 'com.launchdarkly:okhttp-eventsource:2.5.0'
Code
	https://www.conduktor.io/apache-kafka-for-beginners
	
	
	
Wikimedia Producer Implementation
    public static void main(String[] args) throws InterruptedException {
        Properties properties = new Properties();
        properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:9092");
        properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        KafkaProducer<String, String> producer = new KafkaProducer<>(properties);

        EventHandler eventHandler = new WikimediaChangeHandler(producer, "wiki1");
        String url = "https://stream.wikimedia.org/v2/stream/recentchange";
        EventSource.Builder builder = new EventSource.Builder(eventHandler, URI.create(url));
        EventSource eventSource = builder.build();

        // start the producer in another thread
        eventSource.start();

        // we produce for 10 seconds and block the program until then
        TimeUnit.SECONDS.sleep(10);
    }
	public class WikimediaChangeHandler implements EventHandler {
    	private final Logger log = LoggerFactory.getLogger(WikimediaChangeHandler.class.getSimpleName());
	    KafkaProducer<String, String> kafkaProducer;
	    String topic;
	    public WikimediaChangeHandler(KafkaProducer<String, String> kafkaProducer, String topic){
	        this.kafkaProducer = kafkaProducer;
	        this.topic = topic;
	    }
	    @Override
	    public void onOpen() {
	    }
	    @Override
	    public void onClosed() {
	        kafkaProducer.close();
	    }
	    @Override
	    public void onMessage(String event, MessageEvent messageEvent) {
	        System.out.println("onMessage()");
	        log.info(messageEvent.getData());
	        // asynchronous
	        kafkaProducer.send(new ProducerRecord<>(topic, messageEvent.getData()));
	    }
	    @Override
	    public void onComment(String comment) {
	        System.out.println("onComment()");
	    }
	    @Override
	    public void onError(Throwable t) {
	        log.error("Error in Stream Reading", t);
	    }
	}
	
	
	
Wikimedia Producer Config
	Producer Acknowledgement(acks) & Topic Durability
		Producers can choose to recieve acknowledgement of data writes
		acks=0,producer won't wait for acknowledgement(possible data loss)
				Produces the highest throughput setting because network overhead is minimized
		acks=1,producer will wait for leader acknowledgement(limited data loss)
		acks=all(-1), leader + all in-sync-replicas(ISR) acknowledgement(no data loss)
			default for Kafka 3.0+
			The leader replica for a partition checks to see if there are enough ISR 
				for safely writing the message(constrolled by the broker setting min.insync.replicas)
				min.insync.replicas=1 : only the broker leader needs to successfully ack
				min.insync.replicas=2 : at least the broker leader & one replica needs to ack
	Kafka Topic Availability(Considering RF=3)
		acks=0 & acks=1: if one partition is up & considered an ISR, the topic will be available for writes
		acks=all:
			min.insync.replicas=1(default)
				the topic must have at least 1 partittion up as an ISR (that includes the leader) & so we can tolerate two broker being down
			min.insync.replicas=2
				the topic must have at least 2 ISR up & therefore we can tolerate at most one broker being down 
					& we have the gurantee that for every write, the data will be at least written twice
			min.insync.replicas=3
				this wouldn't make much sense for a corresponding RF of 3 & we couldn't tolerate any broker going down
			in summary, when acks=all with a RF of N & min.insync.replicas=M, 
				we can tolerate N-M broker being down for topic availability purpose.
		acka=all & min.insync.replicas=2 is the most popular option for data durability & availability & allows us to withstand at most the loss of 1 kafka broker.		
	Producer Retries
		In case of trasient failures, developer are expected to handle exceptions, otherwise the data will be lost.
		Example of transient failure:
			NOT_ENOUGH_REPLICAS (due to min.insync.replicas setting)
		There is a retries setting
			default to 0 for Kafka <=2.0
			default to 2147483647 for Kafka >=2.1
		The retry.backoff.ms setting is by default 100ms
			100ms will wait before retry
		Producer timeouts
			If retries >0, retry are bounded by a timeout
			Since Kafka>=2.1, delivery.timeout.ms=120000=2min
			Records will be failed if they can't be acknowledged within delivery.timeout.ms
		If we are not using idempotent producer(not recommended-old kafka<1.0.0),
			In case of retries, there is a chance that message will be sent out of order(if a batch has failed to be sent)
			if we rely on  key-based ordering, that can be an issue
			For this, we can set the setting while controls how many produce requests can be made in parallel: max.in.flight.requests.per.connection
				Default:5
				Set to 1 if we need to ensure ordering(it may impact throughput)
			In Kafka >=1.0.0
				There is a better solution with idempotent producers
	Idempotent Producer
		A producer can introduce deuplicate messages in Kafka due to network errors during acks.
		In Kafka >=0.11, we can define a "Idempotent Producer" which won't be introduce duplicates on network error.
			Delete duplicates, dont commit but still acks to producer.
		Idempotent producers are great to guarantee a stable & safe pipeline.
		They are default since Kafka 3.0, recommended to use them
			producerProps.put("enable.idempotence",true);
			This comes with
				acks=all
				retries=Integer.MAX_VALUE (2^31-1=2147483647)
				max.in.flight.requests
					1: for Kafka== 0.11
					5: for Kafka>=1.0, Higher performance& keep ordering- KAFKA-5494
		Safe Kafka Producer Setting
			Since Kafka 3.0 , the producer is safe by default
				acks=all
				enable.idempotence=true
			With Kafka <=2.8, the producer by default   comes with 	
				acks=1
				enable.idempotence=false
			We recommend using a safe producer whenever possible
			Super important: Always use upgraded Kafka Clients
			
		Safe Kafka Producer:Summary
			Since Kafka 3.0,
				The producer is safe by default
			For Kafka <3.0,
				Upgrade your client or set the following settings
				acks=all
					ensure data is properly replicated before an ack is recieved
				min.insync.replicas=2(broker/topic level)
					ensure 2 brokers in ISR at least have the data after an ack
				enable.idempotence=true
					duplicates are not introduced due to network retries
				retreis=MAX_INT(producer level)
					retry until delivery.timeout.ms is reached
				delivery.timeout.ms=120000
					Fail after retrying for 2 min
				max.in.flight.requests.per.connection=5
					Ensure maximum performance while keeping message ordering
				Set safe producer configs (Kafka <= 2.8)
        			properties.setProperty(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
        			properties.setProperty(ProducerConfig.ACKS_CONFIG, "all"); // same as setting -1
        			properties.setProperty(ProducerConfig.RETRIES_CONFIG, Integer.toString(Integer.MAX_VALUE)); // same as setting -1
		Message Compression at Producer Level
			Producer usually send data that is text-based for example with JSON data
			In this case, it is important to apply compression to the producer

			Compression can be enabled at the producer level & doen't require any configuration change in the Brokers or in the Consumers
			compression.type can be none(default), gzip, snappy, zstd(Kafka 2.1)

			Compression is more effective the bigger batch of message being sent to Kafka
			Benchmarks here : https://blog.cloudflare.com/squeezing-the-firehose/ 

			(Message1, Message2,....)->Compressed Producer Batch -> Compressed Messages

			Message Compression Advantages
				The compressed batch has the following advantage:
				Much smaller producer request size(compression ratio up to 4x!)
				Faster to transfer data over the network => less latency
				Better throughput
				Better disk utilization in Kafka (stored messages on disk are smaller)
			Disadvantages(very minor):
				Producer must commit some CPU cycles to compression
				Producer must commit some CPU cycles to decompression
			Overall:
				Consider testing snappy or lz4 for optimal speed /compression ratio(test others too)
				Consider tweaking linger.ms & batch.size to have bigger batches & therfore more compression & higher throughput
				Use compression in Production
		Message compression at the Broker/Topic level
			There is also a setting we can set at the broker level(all topics) or topic-level
			compression.type=producer(default)
				the broker takes the compressed batch from the producer client & writes it directly to the topic's log file without recompressiong the data
			compression.type=none
				all batches are decompressed by the broker
				Inefficient,Not recommended 
			compression.type=lz4(for example)
				If it is matching the producer setting, data is stored on disk as is
				If it is a different compression setting , batches are decompressed by the broker 
					& then re-compressed using the compression algorithm specified
			Warning: If we enable broker-side compression, it will consume extra CPU cycles
 			Rcommendation
 				If we have control over producer 
 					leave compression.type=producer(default) at broker level 
				If we have not control over producer 
					enable compression.type at broker level
		linger.ms & batch.size
			By default, Kafka producers try to send records ASAP
				It will have up to max.in.flight.requests.per.connection=5, 
					meaning up to 5 message-batches being in flight(being sent between producer & broker) at most
				After this, if more messages must be sent while others are in flight, 
					Kafka is smart & will start batching them before the next batch end
			This smart batching helps increase throughput while maintaining very low latency 
				Added benefit: batches have higher compression ratio so better efficiency
			Two settings to influence the batching mechanism
				linger.ms: (default 0) how long to wait until we send a batch.
					Adding a small number for example 5ms helps add more messages in the batch at the expense of latency
				batch.size
					If a batch is filled before linger.ms, increase the batch.size
					Maximum number of bytes that will be included in a batch
					Increasing a batch size to something like 32KB or 64 KB can help 
						increasing the compression, throughput & efficiency of requests
					Any message that is bigger than batch size will not be batched
					A batch is allocated per partition, so make sure that we don't set it to a number that is too high, 
						otherwise we will run waste memory!
					Note: We can monitor the average batch size , metric using Kafka Producer Metrics
		High Throughput Producer
			Increase linger.ms & the producer will wait a few milliseconds for the batches to fill up before sending them
			If you are sending full batches & have memory to spare, you can increase batch.size & send larger batches
			Introduce some producer level compression for more efficiency in sends	
				// set high throughput producer configs(at the expense of a bit of latency & CPU usage)
    		    properties.setProperty(ProducerConfig.LINGER_MS_CONFIG, "20");
        		properties.setProperty(ProducerConfig.BATCH_SIZE_CONFIG, Integer.toString(32 * 1024));//32 KB
        		properties.setProperty(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy");
        		
        	Snappy message compression
        		snappy is very useful if messages are text-based for exammple log lines or JSON documents
        		snappy has a good balance of CPU/compression ratio
        Producer Default Partitioner when key!=null
        	https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner
        	Key Hashing is the process of determining the mapping of a key to a partition
			In the default Kafka partitioner, the key are hashed using the murmur2 algorithm	
				targetPartition=Math.abs(Utils.murmur2(keyBytes))%(numPartitions-1)
			This means that same key will go to the same partition & adding partitions to a topic will completely alter the result partittion
			It is most likely preferred to not override the behaviour of the partitioner, 
				but for advanced cases, it is possible to do so using partitioner.class
        Producer Default Partitioner when key==null
			When key==null, the producer has a defualt partitioner that varies
				Round robin: for Kafka <=2.3
					when there is no partition & no key specified
					This result in more batches(one batch per partition) & smaller batches(imagine with 100 partitions)
					Smaller batches lead to more requests as well as higher latency
				Sticky Partitioner: for Kafka >=2.4
					Sticky Partitioner improves the performance of the producer especially when high throughput when the key is null
        			It would be better to have all the records sent to a single partition & not multiple partition to improve batching
					The producer sticky partitioner:
						We stick to a partition until the batch is full or linger.ms has elapsed
						After sending the batch, the partition that is sticky changes
					Larger batches & reduced latency(because larger requests & batch size more likely to be reached)
					Overtime, records are still spread evenly across partitioners
					Sticky partitioner: Performance improvement
						Latency is noticeably lower 
						Latency is noticeably lower especially when more partitions
		max.block.ms & buffer.memory
			If the producer produces faster than the broker can take, the records will be buffered in memory
			buffer.memory=33554432(32MB): the size of the send buffer
			The buffer will fill up over time & empty back down when the throughput to the broker increases				
			If that buffer is full(all 32MB) then the send() method will start to block(won't return right away)
			max.block.ms=60000, the time the send() will block until throwing an exception
			Exceptions are thrown when:
				The producer has filled up its buffer
				The broker is not accepting any new data
				60 seconds has elapsed
			If you hit an exception hit , that usually means your brokers are down or overloadedas they can't respond to requests
			
					